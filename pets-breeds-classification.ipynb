{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870609df",
   "metadata": {},
   "source": [
    "# Pets Breeds Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1f421",
   "metadata": {},
   "source": [
    "## Import TensorFlow and other libraries\n",
    "\n",
    "<b>Versions used:</b> <br>\n",
    "*Python* - 3.7.0 <br>\n",
    "*Tensorflow* - 2.3.0 <br>\n",
    "*Keras* - 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104b649",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "The datased used in this project, [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), contains photos of 37 different breeds of cats and dogs with roughly 200 images for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
    "data_dir = tf.keras.utils.get_file('images', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad463fc",
   "metadata": {},
   "source": [
    "The downloaded dataset should contain 7,390 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*.jpg'))) + len(list(data_dir.glob('*/*.jpg')))\n",
    "print(str(image_count) + \" images successfully downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07106c9a",
   "metadata": {},
   "source": [
    "Example of an image in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*/*.jpg'))\n",
    "PIL.Image.open(str(images[500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd108364",
   "metadata": {},
   "source": [
    "## Load dataset to a TensorFlow dataset object\n",
    "\n",
    "So far the dataset is just a set of images in a directory. In order to train a model, a *tf.data.Dataset* file have to be created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26310fb7",
   "metadata": {},
   "source": [
    "First adapt directory hierarchy to fit keras  *image_dataset_from_directory* requirements. Create subdirectories for each breed and assign images to appropriate class-directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "breeds = []\n",
    "        \n",
    "for file in data_dir.glob(\"*\"):\n",
    "    if file.suffix == '.jpg':\n",
    "        last_floor_pos = file.name.rfind('_')\n",
    "        breed = file.name[:last_floor_pos].lower()\n",
    "        img_index = file.name[last_floor_pos + 1:]\n",
    "        \n",
    "        if not data_dir.joinpath(breed).is_dir():\n",
    "            data_dir.joinpath(breed).mkdir()\n",
    "            \n",
    "        file.replace(data_dir.joinpath(breed, img_index))\n",
    "        \n",
    "    elif not file.is_dir():\n",
    "        os.remove(file) \n",
    "        \n",
    "for file in data_dir.glob(\"*\"):\n",
    "    breeds.append(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8989f",
   "metadata": {},
   "source": [
    "Define loader parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "split = 0.2       # 80% of the images will be used for training and 20% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e415e5f",
   "metadata": {},
   "source": [
    "Prepare a training data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_count = math.ceil(image_count * (1 - split))\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=split,\n",
    "    subset='training',\n",
    "    seed=2021,\n",
    "    label_mode='categorical',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=train_img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fc96a",
   "metadata": {},
   "source": [
    "Prepare a validation data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_count = image_count - train_img_count\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=split,\n",
    "    subset=\"validation\",\n",
    "    seed=2021,\n",
    "    label_mode='categorical',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=val_img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7103a",
   "metadata": {},
   "source": [
    "## Configure the dataset\n",
    "\n",
    "Add buffered prefetching to ensure the data can be yield from disk without having I/O become blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90768109",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8827d5",
   "metadata": {},
   "source": [
    "Standarise the data to use [0, 1] range, instead of [0, 255] typical for the RGB channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "train_images, train_labels = next(iter(normalized_train_ds))\n",
    "val_images, val_labels = next(iter(normalized_val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8d8f5",
   "metadata": {},
   "source": [
    "Example of an image in standarised dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80119aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_images[33]\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb8eac",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df670b1b",
   "metadata": {},
   "source": [
    "Create data augmentation layer to decrease risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=input_shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd111023",
   "metadata": {},
   "source": [
    "Example of data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = keras.preprocessing.image.img_to_array(image)\n",
    "image_tensor = tf.expand_dims(image_array, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    augmented_image = data_augmentation(image_tensor)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_image[0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def11aa",
   "metadata": {},
   "source": [
    "Load ImageNet pretrained model to implement transfer learning. Use `include_top=False` parameter to remove the last predicting layer of the pretrained model and replace them with own predicting layers. Freeze the weights of the model by setting `trainable=False` of each component layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet=inception_v3.InceptionV3(weights='imagenet', include_top=False)\n",
    "imagenet.summary()\n",
    "\n",
    "for layer in imagenet.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f1973",
   "metadata": {},
   "source": [
    "Define final model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    imagenet,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(37, kernel_initializer='uniform', activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1c4ad",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca65e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cad0f3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59318a0f",
   "metadata": {},
   "source": [
    "## Test the trained model\n",
    "\n",
    "First introduce a helper function to predict a breed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_breed(image_path, best_n = 1):\n",
    "\n",
    "    img = keras.preprocessing.image.load_img(image_path,\n",
    "        target_size=(img_height, img_width))\n",
    "    \n",
    "    img_array = keras.preprocessing.image.img_to_array(img) / 255\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    prediction_probabilities = model.predict(img_array)\n",
    "    top_breeds_indexes = prediction_probabilities[0].argsort()[-best_n:]\n",
    "    \n",
    "    top_predictions = []\n",
    "    for index in top_breeds_indexes[::-1]:\n",
    "        top_predictions.append((breeds[index], prediction_probabilities[0][index]))\n",
    "        \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    return top_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8503a",
   "metadata": {},
   "source": [
    "Let's try a photo of a *german shorthaired*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_image_url = \\\n",
    "    'https://media.nextechclassifieds.com/img/listings/bl/bluelinegundogs/listing_pic_1580328_1580345588.jpeg'\n",
    "dog_image_path = keras.utils.get_file('german_shorthaired',\n",
    "        origin=dog_image_url)\n",
    "\n",
    "\n",
    "predict_breed(dog_image_path, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc705f",
   "metadata": {},
   "source": [
    "Now a *birman* photo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image_url = \\\n",
    "    'https://birman.eu/cms/core_files/thumbs/1500x1100/1.DSC_0344.jpg'\n",
    "cat_image_path = keras.utils.get_file('birman',\n",
    "        origin=cat_image_url)\n",
    "\n",
    "\n",
    "predict_breed(cat_image_path, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c579b3",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "Plot the confusion matrix on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1612d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = model.predict(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a217172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = [np.argmax(row) for row in val_labels]\n",
    "y_pred = [np.argmax(row) for row in model_predictions]\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(breeds))\n",
    "plt.xticks(tick_marks, breeds, rotation=90)\n",
    "plt.yticks(tick_marks, breeds)\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8322da5",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6643d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"model/breeds_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
